{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#背包客棧 取得所有的網址\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "domain='http://www.backpackers.com.tw/forum/'\n",
    "headers={\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.3'\n",
    "}\n",
    "count =0\n",
    "f=open('project5/bag11.txt','w') #開啟一個檔案\n",
    "for i in range(1,64): #抓起1-64頁的所有網址\n",
    "    res=requests.get('http://www.backpackers.com.tw/forum/forumdisplay.php?f=60&prefixid=poi&sort=lastpost&order=desc&daysprune=-1&page={}'.format(i),headers=headers) \n",
    "                     \n",
    "    soup=bs(res.text)\n",
    "    thr=soup.select('#threadbits_forum_60')[0]\n",
    "    for alt1 in thr.select('.alt1'):\n",
    "        if len(alt1.select('a'))>0: #判別如果有網址的話才抓取\n",
    "            if '#' not in str(alt1.select('a')[0]['href']): #過濾有#號的網址\n",
    "                count+=1\n",
    "                print domain+alt1.select('a')[0]['href'],alt1.select('a')[0].text,count  \n",
    "                f.write(domain+alt1.select('a')[0]['href']+'\\n') #存入\n",
    "                time.sleep(1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#背包客棧取得所有原始碼\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "c=1\n",
    "headers={\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.3'\n",
    "} #背包客棧需要使用hearders\n",
    "for li in open('project5/bag11.txt'): #開起每個網址\n",
    "    res=requests.get(li.strip(),headers=headers) \n",
    "    soup=bs(res.text)\n",
    "    tborder=soup.select('.tborder')[6]\n",
    "    try:  #若遇到裡面沒東西則跳過\n",
    "        f=open('project5/bag11/{}.txt'.format(c),'w')#開另一個檔案存取網頁所有的原始碼\n",
    "        print tborder.select('.alt1')[0]\n",
    "        f.write(tborder.select('.alt1')[0].encode('utf-8'))\n",
    "        time.sleep(2)\n",
    "        c+=1\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#背包客棧儲存成資料庫欄位的格式\n",
    "domain='http://www.backpackers.com.tw/forum/showthread.php?s=6ec6162b5de5254263fc62bac5ae53bf&t='\n",
    "ba=[]\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "for j in range(1,1344):\n",
    "    \n",
    "    f=open('project5/bag11/{}.txt'.format(j),'r')\n",
    "    \n",
    "        \n",
    "    a=f.read()\n",
    "    soup=bs(a)\n",
    "    td=soup('td')[0]['id'] #景點id\n",
    "    a11=td.split('_')[2] #\n",
    "    a12=simple2tradition(a11.encode('utf-8'))\n",
    "    print a12\n",
    "    \n",
    "    soup=bs(a) #發文日期\n",
    "    a=soup.select('.smallfont')[0]\n",
    "    b=a.text.split(',')\n",
    "    c=b[0]\n",
    "    m=re.search('.*(20(\\d.*))',c)\n",
    "    b11=m.group(1)\n",
    "    b12=simple2tradition(b11.encode('utf-8'))\n",
    "    print b12\n",
    "    \n",
    "    strong=soup.select('strong') #遊記標題\n",
    "    c11=strong[0].text.split('-')[0]\n",
    "    c12=simple2tradition(c11.encode('utf-8'))\n",
    "    print c12\n",
    "    \n",
    "    vb_postbit=soup.select('.vb_postbit') #遊記內容\n",
    "    d11=vb_postbit[0].text\n",
    "    d12=simple2tradition(d11.encode('utf-8'))\n",
    "    print d12\n",
    "    \n",
    "    \n",
    "    res=str(soup) #來源網址 先把soup轉成字串型式\n",
    "    try:\n",
    "        m2=re.search('<a href=\\\"http:\\/\\/www.facebook.com\\/sharer.php\\?u=http%3A%2F%2Fwww.backpackers.com.tw%2Fforum\\%2Fshowthread.php%3Ft\\%3D(.*?)\\\"',res)\n",
    "    except AttributeError:\n",
    "        print j,'urlempty'\n",
    "    print domain+m2.group(1)\n",
    "    \n",
    "    print '背包客棧'\n",
    "    f=open('project5/bagfinal123/{}.txt'.format(j),'w')\n",
    "    f.write(a12+'||||||||||'+b12+'||||||||||'+c12+'||||||||||'+d12+'||||||||||'+domain+m2.group(1)+'||||||||||'+'背包客棧')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ptt取得所有網址  先在google搜尋列搜尋 台北遊記 site:ptt.cc\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "f=open('project5/ptt.txt','w')\n",
    "domain='www.google.com.tw'\n",
    "count=0\n",
    "for i in range(0,999):\n",
    "    res=requests.get('https://www.google.com.tw/search?q=%E5%8F%B0%E5%8C%97+%E9%81%8A%E8%A8%98+site:ptt.cc&biw=1920&bih=286&ei=0apmVpbzOIWf0gSL6ZiADQ&start={}0&sa=N'.format(i))\n",
    "    soup=bs(res.text)\n",
    "    for g in soup.select('.g'):\n",
    "        m= re.search('[台|臺]北', g.encode('utf-8')) #找尋標題為台北或臺北\n",
    "#        if '台北' in g.select('a')[0].text.encode('utf-8'):\n",
    "        if m:\n",
    "            count+=1\n",
    "            print g.select('cite')[0].text, g.select('a')[0].text, count\n",
    "            f.write(g.select('cite')[0].text+'\\n')\n",
    "            time.sleep(2)\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ptt取得所有網址的原始碼\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "count=588\n",
    "for line in open('project5/ptt.txt'):\n",
    "    count+=1\n",
    "    f=open('project5/ptt/{}.txt'.format(count),'w')\n",
    "    res=requests.get(line.strip())\n",
    "    soup=bs(res.text)\n",
    "    print soup\n",
    "    f.write(soup.encode('utf-8'))\n",
    "    time.sleep(5)\n",
    "    f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ptt儲存成資料庫欄位的格式\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "for i in range(1,781):\n",
    "    f=open('project5/ptt3/{}.txt'.format(i),'r')\n",
    "    a=f.read()\n",
    "    soup=bs(a)\n",
    "    if len(soup.select('.fb-like'))>0:\n",
    "        print soup.select('.fb-like')[0]['data-href'].split('.')[3]  #遊記編號\n",
    "      \n",
    "        \n",
    "        \n",
    "    def funcm(x):   #月份轉換\n",
    "        dic={'Jan':'01','Feb':'02','Mar':'03','Apr':'04','May':'05','Jun':'06','Jul':'07','Aug':'08','Sep':'09','Oct':10,'Nov':11,'Dec':12}\n",
    "        return dic.get(x)\n",
    "\n",
    "    if len(soup.select('.article-meta-value'))>0:\n",
    "        date=soup.select('.article-meta-value')[-1].text.split() #遊記日期\n",
    "        date11=funcm(date[1])\n",
    "        if int(date[2])<10:\n",
    "            date[2]='0'+date[2]\n",
    "        print str(date[4])+str(date11)+str(date[2])\n",
    "    \n",
    "    \n",
    "    if len(soup.select('.article-meta-value')):\n",
    "        print soup.select('.article-meta-value')[-2].text.split()[-1] #遊記名稱\n",
    "    \n",
    "    \n",
    "    if len(soup.select('.bbs-screen'))>0:\n",
    "        bbs=soup.select('.bbs-screen')[0].text.split('--')[0] #遊記內容\n",
    "        #m=re.search('2015(.*)--',bbs)\n",
    "        #print m\n",
    "        print bbs\n",
    "   \n",
    "    if len(soup.select('.fb-like'))>0:    \n",
    "        print soup.select('.fb-like')[0]['data-href']+'||ptt' #文章網址\n",
    "    \n",
    "\n",
    "    if (len(soup.select('.fb-like'))>0 and len(soup.select('.article-meta-value'))>0 and len(soup.select('.article-meta-value')) and len(soup.select('.bbs-screen'))>0 and len(soup.select('.fb-like'))>0):\n",
    "        f=open('project5/pttfinal/{}.txt'.format(i),'w')\n",
    "        f.write(soup.select('.fb-like')[0]['data-href'].split('.')[3].encode('utf-8')+'||||||||||'+str(date[4])+str(date11)+str(date[2]).encode('utf-8')+'||||||||||'+soup.select('.article-meta-value')[-2].text.split()[-1].encode('utf-8')+'||||||||||'+bbs.encode('utf-8')+'||||||||||'+soup.select('.fb-like')[0]['data-href'].encode('utf-8')+'||||||||||ptt'.encode('utf-8'))\n",
    "        f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pixnet取得所有網址\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "f=open('project5/taipei1.txt','w')\n",
    "\n",
    "\n",
    "for i in range(1,999):\n",
    "    res=requests.get('https://www.pixnet.net/searcharticle?q=%E5%8F%B0%E5%8C%97+%E9%81%8A%E8%A8%98&type=related&period=all&page={}'.format(i))\n",
    "    res.encoding='utf-8'\n",
    "    soup=bs(res.text)\n",
    "    inner=soup.select('.inner')[0]\n",
    "    if len(soup.select('.inner')[0])>0:\n",
    "        for li in inner.select('.search-title'):\n",
    "            m= re.search('[台|臺]北', li.select('a')[0].text.encode('utf-8'))\n",
    "            if m:\n",
    "                print li.select('a')[0]['href'], li.select('a')[0].text\n",
    "                f.write(li.select('a')[0]['href']+'||'+str(i)+'\\n')\n",
    "                time.sleep(10)\n",
    "    \n",
    "        \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pixnet取得所有原始碼\n",
    "import requests \n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "count=1\n",
    "domain='http://{}/blog/post/{}'\n",
    "dataload={\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.3'\n",
    "}\n",
    "for i in open('project5/taipei.txt'):\n",
    "    \n",
    "    a=i.split('%2F')\n",
    "#    c=b[2]+'/'+b[3]+'/'+b[4]+'/'+b[5]\n",
    "    b=a[2]\n",
    "    c=a[5].split('-')[0]\n",
    "    res = requests.get(domain.format(b,c),headers=dataload)\n",
    "    res.encoding='utf-8'\n",
    "    soup=bs(res.text)\n",
    "    #aa=soup.select('.article-content-inner')\n",
    "    f=open('project5/pix/{}.txt'.format(count),'w')\n",
    "    print soup\n",
    "    count+=1\n",
    "    f.write(soup.encode('utf-8'))\n",
    "    time.sleep(20)\n",
    "    f.close\n",
    "#    print res1.url\n",
    "#    print res.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pixnet儲存成資料庫欄位的格式\n",
    "#pixnet\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "for i in range(1,5631):\n",
    "    f=open('project5/pix/{}.txt'.format(i),'r')\n",
    "    bb=f.read()\n",
    "    soup=bs(bb)\n",
    "\n",
    "    art=soup.select('.article-content-inner')\n",
    "    if len(art)>0:\n",
    "        print soup.select('.title')[0]['id'].split('-')[1] #遊記編號\n",
    "\n",
    "        def funcm(x):   #月份轉換\n",
    "            dic={'Jan':'01','Feb':'02','Mar':'03','Apr':'04','May':'05','Jun':'06','Jul':'07','Aug':'08','Sep':'09','Oct':10,'Nov':11,'Dec':12}\n",
    "            return dic.get(x)   \n",
    "\n",
    "        year=soup.select('.year')[0].text #年\n",
    "        mon=soup.select('.month')[0].text #月分\n",
    "        aaa=funcm(mon.strip()) #轉換\n",
    "        date=soup.select('.date')[0].text #日\n",
    "        print year.strip()+str(aaa)+date\n",
    "\n",
    "        title=soup.select('.title')[0].text #遊記名稱\n",
    "        print title\n",
    "\n",
    "\n",
    "        art=soup.select('.article-content-inner')[0].text #遊記內容\n",
    "        print art\n",
    "\n",
    "        print soup.select('.title')[0]['data-article-link'] #遊記網址\n",
    "\n",
    "        print 'pixnet'\n",
    "                \n",
    "        f=open('project5/pixfinal/{}.txt'.format(i),'w')\n",
    "        f.write(soup.select('.title')[0]['id'].split('-')[1].encode('utf-8')+'||||||||||'+(year.strip()+str(aaa)+date).encode('utf-8')+'||||||||||'+title.encode('utf-8')+'||||||||||'+art.encode('utf-8')+'||||||||||'+soup.select('.title')[0]['data-article-link'].encode('utf-8')+'||||||||||'+'pixnet'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#以下程式碼皆為抓取網址及標題\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "fid=open(\"wuga.txt\",'w')\n",
    "for i in range(1,3):\n",
    "#while len(soup.select('('a')[-2]['href']'))>0\n",
    "    #url='https://aikolife.com/category/taiwan-travel/taipei-travel/page/'+str(i)\n",
    "    url='http://wuga.com.tw/travel'\n",
    "    res=requests.get(url)\n",
    "    res.encoding='utf-8'\n",
    "#'Page='+str(page)+\n",
    "    soup=bs(res.text)\n",
    "#    domain='http://www.travel.taipei'\n",
    "    for li in soup.select('.scenic_title'):\n",
    "        print li.select('a')[0]['href']\n",
    "        print li.select('a')[0].text.encode('utf-8')+\"\\n\"\n",
    "        #fid.write(li.select('a')[0]['href']+\"\\n\")\n",
    "        #fid.write(li.select('a')[0].text.encode('utf-8')+\"\\n\")\n",
    "time.sleep(3)       \n",
    "fid.close()\n",
    "\n",
    "\n",
    "#抓取台北旅遊網網址\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "fid=open(\"tratpi2.txt\",'w')\n",
    "for page in range(1,4):\n",
    "#while len(soup.select('('a')[-2]['href']'))>0\n",
    "    res=requests.get('http://www.travel.taipei/frontsite/tw/cms/cmsAction.do?recordCount=104&eachRowCount=4&logMenuId=20151&maxRecord=40&menuId=2010701&method=goSpecialCMSList&currentPage='+str(page)+'')\n",
    "    soup=bs(res.text)\n",
    "    domain='http://www.travel.taipei'\n",
    "    for li in soup.select('.textstyle_10'):\n",
    "        fid.write(domain+li.select('a')[0]['href']+\"\\n\")\n",
    "        #fid.write(li.select('a')[0].text.encode('utf-8')+\"\\n\")\n",
    "    time.sleep(2)       \n",
    "fid.close()\n",
    "\n",
    "\n",
    "#抓取wiselyview網站  \n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "fid=open(\"yamx.txt\",'w')\n",
    "a=0\n",
    "for i in range(1,3):\n",
    "#while len(soup.select('('a')[6]['href']'))>0\n",
    "    #url='https://aikolife.com/category/taiwan-travel/taipei-travel/page/'+str(i)\n",
    "    #a=a+10\n",
    "    url='https://www.google.com.tw/search?q=台北遊記+site:http://www.wiselyview.cc&espv=2&start=a&sa=N'\n",
    "    #上一行url中的 &start=a為頁數 a初始值為0 此網站網址第一頁為start=0 第二頁為start=10 第三頁為start=20 所以換頁為+10\n",
    "    a=a+10\n",
    "    res=requests.get(url)\n",
    "    res.encoding='utf-8'\n",
    "#'Page='+str(page)+\n",
    "    soup=bs(res.text)\n",
    "#    domain='http://www.mobile01.com/'\n",
    "    for li in soup.select('h3'):\n",
    "        print li.select('a')[0]['href']\n",
    "        print li.select('a')[0].text.encode('utf-8')\n",
    "        #fid.write(li.select('a')[0]['href']+\"\\n\")\n",
    "        #fid.write(li.select('a')[0].text.encode('utf-8')+\"\\n\")\n",
    "time.sleep(6)       \n",
    "fid.close()\n",
    "\n",
    "\n",
    "#selenium抓取蕃薯藤   以台北 遊記搜尋文章\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "#aLink = 'http://search.yam.com/Search/Mobile/default.aspx?SearchType=blog&k=%e5%8f%b0%e5%8c%97+%e9%81%8a%e8%a8%98&l=0&p=1&'\n",
    "b=1\n",
    "blink = 'http://travel.yam.com/find.aspx?p=b&kw=%E5%8F%B0%E5%8C%97'\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(blink)\n",
    "WebDriverWait(driver, 30).until(lambda driver: driver.find_element_by_id('browserUpdateMsg'))\n",
    "html = driver.page_source\n",
    "cont = str(html.encode('utf-8'))\n",
    "soupCont = BeautifulSoup(cont)\n",
    "    #b=b+1\n",
    "#print soupCont.select('.pageNav a')[12].text\n",
    "while soupCont.select('.pageNav a')[12]>0:  \n",
    "    domain='http://travel.yam.com/'\n",
    "    for li in soupCont.select('.infoListTitle'):\n",
    "        print domain+li.select('a')[0]['href']+\"\\n\"\n",
    "    \n",
    "    driver.find_element_by_link_text(u\"下一頁\").click()\n",
    "\n",
    "driver.close()\n",
    "#driver.quit()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#第二步驟  將抓下來的網址檔yahoo.txt開啟  之後再開新增yahooo資料夾放入網頁原始碼\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "num = 1\n",
    "for line in open('yahoo.txt'):\n",
    "        filename = \"yahoo/{}.txt\".format(num)\n",
    "        fil = open(filename, 'w')\n",
    "        \n",
    "        res = requests.get(line.strip())\n",
    "        res.encoding='utf-8'\n",
    "        soup = bs(res.text)\n",
    "        fil.write(soup.prettify('utf-8'))\n",
    "        num += 1\n",
    "        fil.close()\n",
    "        time.sleep(2)\n",
    "print num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
